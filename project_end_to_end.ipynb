{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This Jupyter Notebook goes through the entire process of building the Image Colorization model. \\\n",
    "First, we generate the training set using colored images, and then we define and train the model. \\\n",
    "Finally, we test our model by colorizing a black and white image.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import img_to_array, load_img, array_to_img\n",
    "from skimage.transform import resize\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, RepeatVector, Reshape, Conv2D, UpSampling2D, concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I. Generate the training set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5 from: https://www.kaggle.com/code/valkling/image-colorization-using-autoencoders-and-resnet/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception = InceptionResNetV2(weights=None, include_top=True)\n",
    "inception.load_weights('inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "inception.graph = tf.compat.v1.get_default_graph()\n",
    "\n",
    "def get_inception_embedding(grey_rgb):\n",
    "    grey_rgb_resized = resize(grey_rgb ,(1, 299, 299, 3))\n",
    "    grey_rgb_resized = preprocess_input(grey_rgb_resized)\n",
    "    embedding = inception.predict(grey_rgb_resized)\n",
    "    return embedding[0]\n",
    "\n",
    "def get_training_data(colored_imgs):\n",
    "    l_imgs, ab_imgs, inception_embeddings= [], [], []\n",
    "    for img in colored_imgs:\n",
    "        gray_img= rgb2gray(img)\n",
    "        gray_rgb_img= gray2rgb(gray_img)\n",
    "        lab_img= rgb2lab(img)\n",
    "        l_img = lab_img[:,:,:1]\n",
    "        assert l_img.shape == (256,256,1), 'L channel matrix has a wrong shape'\n",
    "        ab_img= lab_img[:,:,1:]/128.    # Standardize AB matrices\n",
    "\n",
    "        l_imgs.append(l_img)\n",
    "        ab_imgs.append(ab_img)\n",
    "        inception_embeddings.append(get_inception_embedding(gray_rgb_img))\n",
    "\n",
    "    l_imgs= np.stack(l_imgs, axis=0)  \n",
    "    ab_imgs= np.stack(ab_imgs, axis=0)  \n",
    "    inception_embeddings= np.stack(inception_embeddings, axis=0)\n",
    "    return [l_imgs, inception_embeddings], ab_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Make sure you are refering to the correct path!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download color file from: https://www.kaggle.com/code/theblackmamba31/autoencoder-grayscale-to-color-image/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images file path\n",
    "path= '.'\n",
    "\n",
    "#Standardize RGB images by dividing by 255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#Resize images, if needed\n",
    "train = train_datagen.flow_from_directory(path, classes=['color'], target_size=(256, 256), batch_size=7129, class_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train= 7100 # Number of Training samples\n",
    "\n",
    "X_train, Y_train= get_training_data(train[0][:n_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**II. Colorization Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    embed_input = Input(shape=(1000,))\n",
    "    # Encoder\n",
    "    encoder_input= Input(shape=(256, 256, 1))\n",
    "    encoder_layer= Conv2D(64, (3, 3), activation='relu', padding='same', strides=2)(encoder_input)\n",
    "    encoder_layer= Conv2D(128, (3, 3), activation='relu', padding='same')(encoder_layer)\n",
    "    encoder_layer= Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_layer)\n",
    "    encoder_layer= Conv2D(256, (3,3), activation='relu', padding='same')(encoder_layer)\n",
    "    encoder_layer= Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_layer)\n",
    "    encoder_layer= Conv2D(512, (3,3), activation='relu', padding='same')(encoder_layer)\n",
    "    encoder_layer= Conv2D(512, (3,3), activation='relu', padding='same')(encoder_layer)\n",
    "    encoder_output= Conv2D(256, (3,3), activation='relu', padding='same')(encoder_layer)\n",
    "\n",
    "    # Fusion\n",
    "    # Inception Embedding part\n",
    "    fusion_layer = RepeatVector(32*32)(embed_input) \n",
    "    fusion_layer = Reshape(([32, 32, 1000]))(fusion_layer)\n",
    "    # Adding the Encoder output\n",
    "    fusion_layer = concatenate([encoder_output, fusion_layer], axis=3) \n",
    "    fusion_output = Conv2D(256, (1,1), activation='relu', padding='same')(fusion_layer)\n",
    "\n",
    "    #Decoder\n",
    "    decoder_layer= Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
    "    decoder_layer= UpSampling2D((2,2))(decoder_layer)\n",
    "    decoder_layer= Conv2D(64, (3,3), activation='relu', padding='same')(decoder_layer)\n",
    "    decoder_layer= UpSampling2D((2,2))(decoder_layer)\n",
    "    decoder_layer= Conv2D(32, (3,3), activation='relu', padding='same')(decoder_layer)\n",
    "    decoder_layer= Conv2D(16, (3,3), activation='relu', padding='same')(decoder_layer)\n",
    "    decoder_layer= Conv2D(2, (3,3), activation='tanh', padding='same')(decoder_layer)\n",
    "    decoder_output= UpSampling2D((2,2))(decoder_layer)\n",
    "\n",
    "    return Model(inputs= [encoder_input, embed_input], outputs= decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III. Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model= model()\n",
    "    model.compile(optimizer = Adam(learning_rate= 0.001), loss='mse', metrics = ['acc'])\n",
    "\n",
    "lr_reduction = ReduceLROnPlateau(monitor= 'loss', patience= 3, verbose= 1, factor= 0.5, min_lr= 0.0001)\n",
    "model.fit(X_train, Y_train, validation_split=0.3, epochs= 40, batch_size=32, callbacks=[lr_reduction], verbose= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Uncomment the following line to save your model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('colorizer.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV. Colorize!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(path):\n",
    "    img= img_to_array(load_img(path))/255   # Standardize RGB image array for the RGB-LAB transformation\n",
    "    img = resize(img ,(256,256))    # (256, 256, 3) standard shape for all images\n",
    "\n",
    "    # extract l layer from lab\n",
    "    lab_img= rgb2lab(img)\n",
    "    l_img = lab_img[:,:,:1]\n",
    "    assert l_img.shape == (256,256,1) , \"L channel matrix shape is wrong\"\n",
    "    \n",
    "    input0= l_img.reshape((1,)+ l_img.shape)\n",
    "\n",
    "    # generate gray image \n",
    "    gray_img= rgb2gray(img)\n",
    "    gray_rgb_img= gray2rgb(gray_img)\n",
    "\n",
    "    input1= get_inception_embedding(gray_rgb_img)\n",
    "    input1= input1.reshape((1,)+ input1.shape)\n",
    "    assert input1.shape == (1, 1000) , \"input 2 shape is wrong\"\n",
    "\n",
    "    input= [input0, input1]\n",
    "    ab_predict= model.predict(input)[0]*128\n",
    "    assert ab_predict.shape == (256,256,2) , \"AB channels matrices shape is wrong\"\n",
    "\n",
    "    result_lab= np.zeros(img.shape)\n",
    "    result_lab[:,:,:1], result_lab[:,:,1:]= l_img, ab_predict    # Give backthe original values range for AB\n",
    "    result_rgb= lab2rgb(result_lab)\n",
    "\n",
    "    return array_to_img(result_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Give the path of your image*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path= ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if img_path == '':\n",
    "    print('Give a correct image path')\n",
    "else:\n",
    "    img= colorize(img_path)\n",
    "    plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
